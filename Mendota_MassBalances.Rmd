---
title: "Mendota_MassBalances"
author: "Lauren A. Knose, ORISE-EPA"
date: '2023-02-14'
output: html_document
---

The purpose of this program is to calculate the mass balances for each pool of 
Phosphorus (P) in Lake Mendota, WI, during the Summer. 

Step 1. set working directory and load dependent packages:

```{r}
library(ggplot2) #needed for plots
library(ggbreak) #needed for plot scale breaks
library(dplyr) #needed for reshaping/reformaing data
library(lubridate) #needed for reformatting data and using "year" function
library(rayshader) #needed to make 3D plots
```

# Step 2. Establish model terms and load the data into R: 

Setting up model terms...

```{r}
res_ti = 4.5 #residence time in Lake Mendota in years
t_res = 14 #number of days between sample points
mo_day = 1*30 #conversion factor from month to days
day_YR = 1/365 #conversion factor from days to year
km2_m2= 1*1000000 #conversion factor from km^2 to m^2
L_m3 = 1/1000 #conversion factor from liters to m^3
mg_g = 1/1000 #conversion factor from mg to g
g_kg = 1/1000 #conversion factor from g to kg
cm3_m3 = 1/1000000 # conversion factor from cm^3 to m^3
cfs_cms = 1/35.3147 #conversion factor from ft^3/sec to m^3/sec
cms_m3day= (60/1)*(60/1)*(24/1)  #conversion factor from m^3/sec to m^3/day

Z_avg = 12.8 #average depth of Lake Mendota, WI in meters
Z_max_m = 25.3 #max depth of Lake Mendota, WI in meters
Z_cd_m = 3.59 #compensation depth in m
Z_epi_m = Z_cd_m #depth above compensation depth (epilimnion + metalimnion)
Z_hypo_m = Z_max_m - Z_epi_m #depth below compensation depth ()
Z_se_m = 0.1 #depth of unconsolidated sediments in meters (from Nurnberg 1988)

A_lake_m2 = 39.40*km2_m2 #surface area of Lake Mendota in m^2
R_upper = sqrt((LSA*km2_m2)/pi) # calculates the radius of the upper layer
r_lower = 1306 # calculates the radius of the lower layer = 1306 m
A_sed<- pi*(r_lower^2) #area of sediments in m^2

V_epi_m3 = 141307680.85 #volume of epi layer in m^3
V_hypo_m3 = 181506569.09 #volume of hypo layer in m^3
V_sed_m3 = 11820000 #volume of sediment (solids + water) in m^3

D_sed_gm3 = 1330000 #standard bulk density for clay/silt from USDA in g/m^3
D_wat_gm3 = 1*cm3_m3 #standard density for water (1 g/cm^3) in g/m^3
gsed_gP = 0.806/1000 #mass of available P in 1 g of sediment dry weight (g/g)

g_moleC = 12.011 #g per mole of C
g_moleN = 14.007 #g per mole of N
g_moleP = 30.974 #g per mole of P
moleC_molePhyto = 106 #moles of C per 1 mole of phytoplankton organic matter from            #Redfield's ratio of all algal mass (C_106*H_263*O_110*N_16*P_1)
moleN_molePhyto = 16 #moles of N per 1 mole of phytoplankton OM
moleP_molePhyto = 1 #moles of P per 1 mole of phytoplankon OM
mmole_gP= (1/1000)*g_moleP #conversion factor from milimoles to g P
```

Model terms defined.

Load in the data...

```{r}
NTL_chem<-read.csv(file.choose()) #Water column P data, in Original_data folder,
# choose ntl1_v9_1_chem.csv
```

Water column nutrient concentrations from the center of the lake loaded (source:
North Temperate Lakes Project).

```{r}
inflowA<- read.csv(file.choose(), header=TRUE) #Pheasant branch inflow data in Cleaned data Folder
inflowB<- read.csv(file.choose(), header=TRUE) #Yahara River inflow data in cleaned data folder
inflowC<- read.csv(file.choose(), header=TRUE) #Six-mile Creek inflow data in Cleaned data folder
```

Flow rate and P concentration for two major inflows loaded. Note, some P values
are interpolated from source (Hanson et al 2020).

```{r}
outflow<- read.csv(file.choose(), header=TRUE) #, in Original_data folder. choose Mendota_Export_Tenneydam.csv
```

Outflow flow data from Tenney dam loaded. P concentration in outflow are 
not available (source: USGS).


# Step 3. Filter and transform data:

From my volume calculations, I learned that the Secchi disk depth, on average, doesn't significantly change from day 195 (mid-July) to day 255 (mid-September). Cyanobacteria relative biomass remains > 75% during this period. This means I should use this date range to represent the CyanoHAB season. 

TP and SRP are both collected at the following depths in meters from the surface:
`r unique(sum_ME_TP$depth)`. Since the compensation depth 
is 3.5 m, I can average the measurements from 0 to 4 m  for the epilimnion, and 
average the measurements from 4 m to max depth for the hypolimnion.   

Creating data file with total P (both concentration and mass) in epi:
```{r}
P_epi<- NTL_chem %>%
  filter(lakeid=="ME", #filter for Lake Mendota
         depth <"4", #filter for measurements taken above 4m
         rep=="1") %>% #filter for rep 1 data only (exclude analytical reps)
  mutate(sampledate=as.Date(sampledate, origin="1899-12-30"), #format as Date
         TotP_mgL=totpuf_sloh, #rename total P field with units (mg/L)
         P_kg=TotP_mgL*mg_g*(1/L_m3)*V_epi_m3*g_kg) %>% 
          #add field with calculated P in kg
  select(year4, daynum, sampledate, TotP_mgL, P_kg, depth) %>%#select only fields needed
  group_by(sampledate) %>% #group all observations taken on same sample date
  summarise(meanTP_mgL= mean(TotP_mgL, na.rm=TRUE), #calculate mean total P
            sdTP_mgL= sd(TotP_mgL, na.rm=TRUE), #calculate stdev total P
            meanP_kg = mean(P_kg, na.rm=TRUE), #calculate mean diss P
            sdDP_kg = sd(P_kg, na.rm=TRUE)) %>% #calculate stdev diss P
  mutate(year4=year(ymd(sampledate)), #adds back field with year
         daynum=format(sampledate, "%j")) #adds back field with Julian day
```

Data with epi P ready.

Creating data file with total P (both concentration and mass) in hypo:
```{r}
P_hypo <- NTL_chem %>%
  filter(lakeid=="ME", #filter for Lake Mendota
         depth >= "4", #filter for measurements taken below 4m
         rep=="1") %>% #filter for rep 1 data only (exclude analytical reps)
  mutate(sampledate=as.Date(sampledate, origin="1899-12-30"), #format as Date
         TotP_mgL=totpuf_sloh, #rename total P field with units (mg/L)
         P_kg=TotP_mgL*mg_g*L_m3*V_epi_m3*g_kg) %>% 
          #add field with calculated P in kg
  select(year4, daynum, sampledate, TotP_mgL, P_kg, depth) %>% #select only fields needed
  group_by(sampledate) %>% #group all observations taken on same sample date
  summarise(meanTP_mgL= mean(TotP_mgL, na.rm=TRUE), #calculate mean total P
            sdTP_mgL= sd(TotP_mgL, na.rm=TRUE), #calculate stdev total P
            meanP_kg = mean(P_kg, na.rm=TRUE), #calculate mean diss P
            sdDP_kg = sd(P_kg, na.rm=TRUE)) %>% #calculate stdev diss P
  mutate(year4=year(ymd(sampledate)), #adds back field with year
         daynum=format(sampledate, "%j")) #adds back field with Julian day
```

Data with hypo P ready.

Creating data file with only epi P data during CyanoHAB season

```{r}
epi_sum_P <- P_epi %>%
  filter(daynum >= 195 & daynum < 255) #filter for data during CyanoHAB season
```

Summer epi P data ready. 

```{r}
hypo_sum_P <- P_hypo %>%
  filter(daynum >= 195 & daynum < 255) #filter for data during CyanoHAB season
```

Summer hypo P data ready.

Transform the inflow data from Pheasant Run stream:

```{r}
PheasInflow <- inflowA %>%  #inflow data from Pheasant Branch river
  mutate(TotP_gm3=(TotP_mgL*mg_g*(1/L_m3)), #calculate total P as g/m^3
         Qin_m3day=(Qout_cfs*cfs_cms*cms_m3day),#calculate flow rate in m^3/day
         Pload_gday= (TotP_gm3*Qin_m3day), #units in g/day
         sampledate=as.Date(sampledate, format="%m/%d/%Y",
                            origin="1899-12-30"), #tells R to format 
          #data as date
         daynum=format(sampledate, "%j")) %>% #adds field with Julian day
  select(sampledate, daynum, TotP_gm3, Qin_m3day, Pload_gday) %>%#select fields 
  filter(!is.na(Pload_gday)) #remove NAs
```

Inflow A (Pheasant Branch stream) total P loading ready. 

Transform the data from Yahara River stream:

```{r}
YaharInflow <- inflowB %>%  #inflow data from Yahara River
  mutate(TotP_gm3=(TotP_mgL*mg_g*(1/L_m3)), #calculate total P as  g/m^3
         Qin_m3day=(Q_cfs*cfs_cms*cms_m3day),# units in m^3/day
         Pload_gday= (TotP_gm3*Qin_m3day) , #                                                   units in g/day
         sampledate=as.Date(sampledate, format="%m/%d/%Y",
                            origin="1899-12-30"), #tells R to format 
          #data as date
         daynum=format(sampledate, "%j")) %>% #adds field with Julian day
  select(sampledate, daynum, TotP_gm3, Qin_m3day, Pload_gday) %>%#select fields 
  filter(!is.na(Pload_gday)) #remove NAs
```

Inflow B (Yahara River stream) total P loading ready. 

Transform the data from Six-mile Creek stream:

```{r}
SixMileInflow <- inflowC %>%  #inflow data from Yahara River
  mutate(TotP_gm3=(TotP_mgL*mg_g*(1/L_m3)), #calculate total P as  g/m^3
         Qin_m3day=(Q_cfs*cfs_cms*cms_m3day),# units in m^3/day
         Pload_gday= (TotP_gm3*Qin_m3day) , #                                                   units in g/day
         sampledate=as.Date(sampledate, format="%m/%d/%Y",
                            origin="1899-12-30"), #tells R to format 
          #data as date
         daynum=format(sampledate, "%j")) %>% #adds field with Julian day
  select(sampledate, daynum, TotP_gm3, Qin_m3day, Pload_gday) %>%#select fields 
  filter(!is.na(Pload_gday)) #remove NAs
```

Inflow C (Six-Mile Creek stream) total P loading ready.

Combine the three inflow data:

```{r}
inflow_comb <- merge(PheasInflow, YaharInflow, 
                     by ="sampledate") #merge the two inflow data files
inflow_comb <- inflow_comb %>%
  mutate(TotPload_gday=(Pload_gday.x+Pload_gday.y),
         daynum=daynum.x,
         year4=year(ymd(sampledate))) %>% #combine P loads
  select(c(sampledate, daynum, TotPload_gday, year4)) #select fields needed
inflow_comb<- merge(inflow_comb, SixMileInflow, by="sampledate")
inflow_comb <- inflow_comb %>%
  mutate(TotPload_gday=(TotPload_gday+Pload_gday),
         daynum=daynum.x,
         year4=year(ymd(sampledate))) %>% #combine P loads
  select(c(sampledate, daynum, TotPload_gday, year4)) #select fields needed
```

Inflow total P loading combined into one data file "inflow_comb". 

Transform the outflow data (only 1 main stream outflow):

```{r}
outflow_Q<- outflow %>%  
  filter(!is.na(Q_cfs)) %>%
  mutate(sampledate=as.Date(datetime, 
                            format="%m/%d/%Y", #tells R what format the date is in
                            origin="1899-12-30"),
         year4=year(ymd(sampledate)), #adds field with year
         daynum=format(sampledate, "%j"), #adds back the field with Julian day
         Qout_m3day=(Q_cfs*cfs_cms*cms_m3day)) %>%# units in m^3/day)  
  select(sampledate, Qout_m3day, year4, daynum) #select fields needed
outflow_P <- merge(outflow_Q, P_epi, by="sampledate") #merge flow and P data
outflow_P <- outflow_P %>%
  mutate(TotPexp_gday=(meanTP_mgL*mg_g*(1/L_m3)*Qout_m3day),#calculates total P exported g/day
         daynum=daynum.x,
         year4=year4.x) %>% #combine P loads
  select(sampledate, year4, daynum, Qout_m3day, TotPexp_gday) #select fields
#write.csv(outflow_P, "outflow_TotP.csv") #use this code first time to save the new data file
```

Outflow total P export data ready. Note, Outflow data only available from `r min(outflow_P$sampledate)` to `r max(outflow_P$sampledate)`.

Calculate the P stored (P_stored = P_extloading - P_extexport). If
P_stored is positive, then P_extloading > P_extexport. If P_stored is negative, 
then, P_extloading < P_extexport. This means that the extra P leaving the system 
must have come from internal loading.

```{r}
P_inout<- merge(inflow_comb, outflow_P, by= "sampledate")
P_inout<- P_inout %>%
  filter(!is.na(TotPexp_gday)) %>%
   mutate(daynum=daynum.x,
          year4=year4.x,
          P_stored_gday=(TotPload_gday-TotPexp_gday),
          P_netintload_gday=(TotPexp_gday-TotPload_gday)) %>%
  select(sampledate, daynum, year4, TotPload_gday, TotPexp_gday,
         P_stored_gday, P_netintload_gday)
#write.csv(P_inout, "TotP_inout.csv") #use when first time running program
```

P stored calculated from P inflow and P export.

An important thing to keep in mind is that sampling may not occur exactly on a
regular schedule, due to inclement weather, access issues, or personnel available.
If measurements were taken bimonthly from 1995 to 2018 (24 years), there was 
observation for around Jul 15, Jul 30, Aug 15, Aug 30, and Sep ~15. 
I should have 5 observations per year * 24 years for a total of 120 observations. 
Since I don't have a matching number of records, this means I do not have the 
same number of observations for each year. In order to do time series, I need to 
have consistent time periods between observations. To solve this challenge I need 
to go through each year and assign the approximate Julian day and interpolate 
any missing values. I exported the data to Excel, added a new column "Use_Jdate" 
with either 196, 211, 227, 242, or 258. I then performed a linear regression of 
the data from each year there was a missing observation and added the estimated
value. I performed this protocol for epi TP, epi SRP, hypo TP, and hypo SRP.
If you don't want to use the corrected J date, use the sample date instead.

Creating the corrected J date and interpolation of missing data (do in excel)...

```{r}
# write.csv(epi_sum_ME_TP, file="epi_sum_ME_TP.csv")
## performed further clean-up of data in excel --> see lab notes for 2/15/2023
```

Loading in the new data file with corrected J date and interpolated data...

```{r}
epi_sum_ME_TP <- read.csv(file.choose(), header=TRUE) # select the csv file
## from Cleaned_data folder 
```

New data file has corrected J dates and interpolated missing data.


Creating the corrected J date and interpolation of missing data (do in excel)...

```{r}
#write.csv(epi_sum_ME_SRP, file="epi_sum_ME_SRP.csv")#filter for measurements taken from 0m
## performed further clean-up of data in excel --> see lab notes for 2/15/2023
```

Loading in the new data file with corrected J date and interpolated data...

```{r}
epi_sum_ME_SRP <- read.csv(file.choose(), header=TRUE) # select the csv file
## from Cleaned_data folder 
## I now have all complete cases
```

New data file has corrected J dates and interpolated missing data.

Creating the corrected J date and interpolation of missing data (do in excel)...

```{r}
#write.csv(hypo_sum_ME_TP, file="hypo_sum_ME_TP.csv")#filter for measurements 
## taken from 0m, performed further clean-up of data in excel 
## --> see lab notes for 2/15/2023
```

Loading in the new data file with corrected J date and interpolated data...

```{r}
hypo_sum_ME_TP <- read.csv(file.choose(), header=TRUE) # select the csv file
## from Cleaned_data folder 
## I now have all complete cases
```
New data file has corrected J dates and interpolated missing data.

Creating the corrected J date and interpolation of missing data (do in excel)...

```{r}
#write.csv(hypo_sum_ME_SRP, file="hypo_sum_ME_SRP.csv")#filter for measurements taken from 0m
## performed further clean-up of data in excel --> see lab notes for 2/15/2023
```

Loading in the new data file with corrected J date and interpolated data...

```{r}
hypo_sum_ME_SRP <- read.csv(file.choose(), header=TRUE) # select the csv file
## from Cleaned_data folder 
## I now have all complete cases
```

New data file has corrected J dates and interpolated missing data.

# Step 4.  Summarize and view the data:

Plot P in g in epilimnion over the years:

```{r}
ggplot(data=P_epi, 
       aes(x=as.numeric(daynum), y=meanTP_mgL*1000)) + 
  geom_line() +
  facet_wrap(~year4) +
  labs(title="Epilimnion, Lake Mendota, WI", x="Day of Year", 
       y="Total P (ug/L)",
       caption="Grey shading indicates CyanoHAB season.") +
  theme_classic()+
  annotate('rect', xmin=195, xmax=255, ymin=0, ymax=Inf, alpha=.2, fill='gray60') +
  geom_hline(yintercept=20, lty = "dashed") 
```

Plot P in g at hypo over the years:
```{r}
ggplot(data=P_hypo, 
       aes(x=as.numeric(daynum), y=meanTP_mgL*1000)) + 
  geom_line() +
  facet_wrap(~year4) +
  labs(title="Hypolimnion, Lake Mendota, WI", x="Day of Year", 
       y="Total P (ug/L)",
       caption="Grey shading indicates CyanoHAB season.") +
  theme_classic()+
  annotate('rect', xmin=195, xmax=255, ymin=0, ymax=Inf, alpha=.2, fill='gray60') +
  geom_hline(yintercept=20, lty = "dashed")  
```

Plot P external loading from inflows across time:

```{r}
ggplot(data=subset(P_inout, year4>=2004), 
       aes(x=as.numeric(daynum), y=TotPload_gday/1000)) + 
  geom_point() + #makes a scatterplot
  facet_wrap(~year4, scales="free") + 
  theme_classic() + #color graph as classic figure
  labs(x="Day of Year", y=" Total P External Loading (kg/day)",#adds axis labels
       title="Lake Mendota, WI",
       caption="Source: Lake Mendota,North Temperate Lakes Dataset. \n Grey shading indicates CyanoHAB season.") +
  annotate('rect', xmin=195, xmax=255, ymin=0, ymax=Inf, alpha=.2, fill='gray60') 
```

Plot P in outflow over the years:

```{r}
ggplot(data=subset(P_inout, year4>=2004), 
       aes(x=as.numeric(daynum), y=TotPexp_gday/1000)) + 
  geom_point() + #makes a scatterplot
  facet_wrap(~year4, scales="free") + 
  theme_classic() + #color graph as classic figure
  labs(x="Day of Year", y=" Total P Exported (kg/day)",#adds axis labels
       title="Lake Mendota, WI",
       caption="Source: Lake Mendota,North Temperate Lakes Dataset, dashed lines \n indicate CyanoHAB season.") +
  annotate('rect', xmin=195, xmax=255, ymin=0, ymax=Inf, alpha=.2, fill='gray60')   
```

The figure above indicates earlier years (< 2010s) had greater P export than 
later years (> 2010s).

Plot the Difference in P loading from inflows and P export in outflow:

```{r}
ggplot(data=P_inout, 
       aes(x=as.numeric(daynum), y=P_stored_gday/1000)) + 
  geom_point() +
  facet_wrap(~year4, scales="free") +
  labs(main="Lake Mendota, WI", x="Day of Year", 
       y="Total P (kg/day) either stored (+) or released (-)",
       caption="Grey shading indicates CyanoHAB season.") +
  theme_classic()+
  annotate('rect', xmin=195, xmax=255, ymin=0, ymax=Inf, alpha=.2, fill='gray60') +
  geom_hline(yintercept=0, lty = "dashed") 
```


Graphing average total P in the epilimnion, mid-July to mid-Sept

```{r}
ggplot(data=epi_sum_ME_TP, aes(Use_Jdate, totpuf_sloh, color=as.factor(year4))) + 
  geom_point() +
  geom_smooth(method="loess") + 
  theme_classic() +
  theme(legend.justification=c("right", "top")) +
  labs(x="Day of Year", y=" Total P (mg/L)",#adds axis labels
       title="Summer Epilimnion, mid-July to mid-September, Lake Mendota, WI",
       caption="Source: Lake Mendota,North Temperate Lakes Dataset",
       color="Year")
```

The figure above tells us that there is relatively high variability in total P
between years.The two years with the highest total P in epilimnion were 2008 and
2009, respectively. The latest years remained low relative to prior years. 


Graph total P in hypo during CyanoHAB season:

```{r}
ggplot(data=subset(hypo_sum_ME_TP, year4>2003 & year4<2016), #subset the data
       aes(x=as.numeric(Use_Jdate), y=mean_hypo_TP, color=as.factor(year4))) + 
  geom_line() +
  facet_wrap(~year4)+
  theme_classic() +
  labs(x="Day of Year", y="Total P in Hypo (mg/L)",
       title="Lake Mendota, WI, mid-July to mid-September",
       caption="Source: North Temperate Lakes Dataset",
       color="Year")
```

The figure above demonstrates more clearly these peaks (pulses) and valleys
of total P in the hypolimnion, within the Summer period. The peaks of TP may be 
attributed to pulses from the sediments. Interestingly, this pattern is inverse
depending on the year.


# Step 5. Calculate internal P load:
To calculate internal load, as described in Nurnberg and LaZerte 2016, I need to 
define some additional terms and functions.


Calculating internal P load via Nurnberg and LaZerte calculations:

```{r}
P_anL <- P_inout %>%
  #filter(year4>2003 & year4 < 2016) %>% # remove the one observation in 2003
  group_by(year4) %>% #for each year
  summarise(TotPload_gyr=sum(TotPload_gday), #sums total g P for yr
            TotPexp_gyr=sum(TotPexp_gday)) #sums total g P for yr
P_NetIL <- P_anL %>%  
  mutate(L_ext = TotPload_gyr/A_lake_m2, #calculates annual areal
         # external P load in g/m^2-yr
         L_out = TotPexp_gyr/A_lake_m2, #calculates annual areal
         #export of P in g/m^2-yr
         R_meas= (L_ext - L_out)/L_ext)  #the proportion of P retained in lake 
          # each year

outflow_Avg <- outflow_P %>%
  #filter(year4>2003 & year4 < 2016) %>% # filter for years with complete data
  group_by(year4) %>% #for each year
  summarise(AvgQ_m3day = mean(Qout_m3day, na.rm=TRUE)) #calculate the average outflow vol
outflow_Qs <- outflow_Avg %>%
  mutate(Qs = (AvgQ_m3day*365/A_lake_m2), #calculate the annual water load in m/yr
         R_pred = (12/(18 + Qs))) #calculates teh predicted retention (downward 
          #flux of P due to settling and sedimentation)
P_NetIL<- merge(P_NetIL, outflow_Qs, by="year4")
P_NetIL <- P_NetIL %>%
  mutate(Net_IL_mgm2yr=(L_ext*R_pred)-(L_ext*R_meas), #net internal loading in mg/m^2-yr
         Net_IL_gyr=Net_IL_mgm2yr/1000*A_lake_m2)
write.csv(P_NetIL, file="P_NetIL.csv")
```

Annual internal P loading is now calculated. 

Below is the graph of the internal P load by year:

```{r}
ggplot(data=P_NetIL, #subset the data
       aes(x=year4, y=Net_IL_gyr)) + 
  geom_point()+ 
  geom_line() +
  theme_classic() +
  labs(x="Year", y="Internal P Loading (g/yr)",
       title="Lake Mendota, WI")
```

# Step 6. Calculate the mass for each pool at each time point.


```{r}
epi_M_TP <- epi_sum_ME_TP %>% #mass of TP in epi
  mutate(massTP_epi_kg = ((totpuf_sloh*L_m3*mg_g*g_kg)*V_epi_m3))

hypo_M_TP <- hypo_sum_ME_TP %>% #mass of TP in hypo
  mutate(massTP_hypo = ((mean_hypo_TP*L_m3*mg_g*g_kg)*V_hypo_m3)) #in kg

M_sed_g <- (D_sed_gm3*gsed_gP)*V_sed_m3 #mass of P per vol of sediment in g/m^3
```

The mass of total phosphorus in the epilimnion during the summer is as follows:
```{r}
plot(epi_M_TP$massTP_epi_kg~ epi_M_TP$daynum, 
     xlab="Day of Year", ylab="Total Phosphorus (kg)",
     main="Summer Epilimnion, Lake Mendota, WI")
```

The mass of total phosphorus in the hypolimnion during the summer is as follows:

```{r}
plot(hypo_M_TP$massTP_hypo~ epi_M_TP$daynum, 
     xlab="Day of Year", ylab="Total Phosphorus (kg)",
     main="Summer Epilimnion, Lake Mendota, WI")
```


#Step 7. Calculate the mass balances for each pool, starting with epi, then hypo,
then sediments.


dPepi/dt = [Load of dissolved P] + Load from Entrainment - Loss from Sedimentation - Loss from Export
Where…
Load total P = [daily inflow in m3/day] * [Pstream in mg/m3]
Load of dissolved P = [Load total P in mg/day] * (1 - 0.5)
Load from Entrainment = [change in Epi daily volume in m3/day ] * [Pepi in mg/m3]
Loss from Sedimentation = sum( [load total P * 0.5) + ([Pepi] * 0.0137*(1.065^Tepi - 10 C))
Loss from Export = [daily outflow in m3/day] * [Pepi in mg/m3]

I need the daily inflow, [P] in inflow, [P] in epi, Temp of epi, daily outflow, 
and [P] in outflow.

```{r}
plot(inflow_comb$sampledate, inflow_comb$TotPload_gday)
plot(outflow_P$sampledate, outflow_P$TotPexp_gday)
```

Look at inflow P loading averaged by Julian day:



epi_sum_ME_TP %>%
  mutate(Pepi=totpuf_sloh*L_m3)



temp_epi<- read.csv(file.choose(), header=TRUE)
outflow<- read.csv(file.choose(), header=TRUE)

cfs_m3day<- (86400/35.3147) #conversion factor from ft3/sec to m3/day
outflow<- outflow %>%
  mutate(discharge_m3day=discharge_cfs*cfs_m3day) %>% # recalculates discharge
  mutate(sampledate=as.Date(datetime, origin="1899-12-30"), #tells R to format 
          #data as date
         daynum=format(sampledate, "%j")) #adds field with Julian day

outflowP<- read.csv(file.choosE(), header=TRUE)


dPhypo/dt = Load from Recycling - Loss from Entrainment
Where…
Load from recycling = [Lake Area of hypo in m2] * (-4.3 mg P/m2-day + ([22.86 g dry weight/m2-day] * [0.806 mg P per g sediments])


A_hypo<- (V_hypo_m3/Z_hypo_m) #calculates are of hypo in m2
Recycling<- A_hypo*(-4/3 + (22/86*0.806))
dPhypo_dt = Recycling - Entrainment

dPsed/dt = LoadpartP + Load from Sedimentation - Loss from Recycling - loss from Burial
LoadpartP = [Load total P in mg/day] * 0.5
#Burial = [1.2 mm/yr] * (1yr/365 days) * ([22.86 g dry weight/m2-day] * [0.806 mg P per g sediments]) * 
(0.1 m)

```{r}
Burial<- 1.2*(1/365)*(22.86*0.806*(1/0.1))/1000
<- Burial *V_sed_m3
```


